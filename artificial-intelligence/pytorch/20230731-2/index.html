<!-- build time:Fri Jun 21 2024 16:44:48 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="学无止境" href="https://blog.aayu.today/rss.xml"><link rel="alternate" type="application/atom+xml" title="学无止境" href="https://blog.aayu.today/atom.xml"><link rel="alternate" type="application/json" title="学无止境" href="https://blog.aayu.today/feed.json"><link rel="stylesheet" href="/assets/fonts.googleapis.com.css"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="PyTorch"><link rel="canonical" href="https://blog.aayu.today/artificial-intelligence/pytorch/20230731-2/"><title>PyTorch学习笔记（10）长短期神经网络LSTM - 机器学习修炼之PyTorch - 人工智能 | Aayu Yain = 学无止境 = 世界上大部分事，都没太大意义。真理与热爱除外</title><meta name="generator" content="Hexo 5.4.2"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">PyTorch学习笔记（10）长短期神经网络LSTM</h1><div class="meta"><span class="item" title="创建时间：2023-08-01 00:52:36"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-08-01T00:52:36+08:00">2023-08-01</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>3.5k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>3 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Aayu Yain</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://image.aayu.today/uploads/2022/12/28/6833939bly1gicitf0kl1j20zk0m87fe.jpg"></li><li class="item" data-background-image="https://image.aayu.today/uploads/2022/12/28/6833939bly1giciundwu5j20zk0m8n9e.jpg"></li><li class="item" data-background-image="https://image.aayu.today/uploads/2023/08/24/202308242324684.png"></li><li class="item" data-background-image="https://image.aayu.today/uploads/2023/08/09/202308092219016.png"></li><li class="item" data-background-image="https://image.aayu.today/uploads/2022/12/28/6833939bly1giciukx8a7j20zk0m8aio.jpg"></li><li class="item" data-background-image="https://image.aayu.today/uploads/2022/12/28/6833939bly1giph4wqtg4j20zk0m8x6p.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/artificial-intelligence/" itemprop="item" rel="index" title="分类于 人工智能"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/artificial-intelligence/pytorch/" itemprop="item" rel="index" title="分类于 机器学习修炼之PyTorch"><span itemprop="name">机器学习修炼之PyTorch</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.aayu.today/artificial-intelligence/pytorch/20230731-2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="宇凌喵"><meta itemprop="description" content="世界上大部分事，都没太大意义。真理与热爱除外, 真理和热爱是吾永生的追求"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="学无止境"></span><div class="body md" itemprop="articleBody"><p>最为关键的点就在于 LSTM 有三个门，遗忘门（forget gate）、输入门（input gate）和输出门（output gate），每个门通过 sigmoid 激活函数（σ）输出 0 或者 1，然后通过 element-wise 的乘积操作，达到筛选信息的目的。</p><p><img data-src="assets/image-20230801000723-64t8wf4.png" alt="image"></p><h2 id="遗忘门"><a class="anchor" href="#遗忘门">#</a> 遗忘门</h2><p><img data-src="assets/image-20230801000935-ct94029.png" alt="image"></p><p>选择性的筛选上一步的记忆信息 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.891661em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:-.07153em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span></span></span></span>，所以叫做遗忘门</p><h2 id="输入门"><a class="anchor" href="#输入门">#</a> 输入门</h2><p><img data-src="assets/image-20230801001536-y6b1u83.png" alt="image"></p><p>将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.902771em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 的经过 Sigmoid 激活函数得到输入门，同时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.902771em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 经过 tanh 激活函数得到新的记忆信息，然后和输入门经过 element-wise 的乘积操作，将输入进行选择性的筛选，所以叫做输入门</p><p><img data-src="assets/image-20230801002028-83zq08e.png" alt="image"></p><p>接下来就是更新上一步的记忆信息 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">C_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.891661em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:-.07153em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span></span></span></span>，通过 element-wise 的相加操作</p><p>从这里也可以看出输入门和输出门的状态组合会导致不同的结果</p><table><thead><tr><th style="text-align:center">input gate</th><th style="text-align:center">forget gate</th><th style="text-align:center">behavior</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">remember the previous value</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">add to the previous value</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">erase the value</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">overwrite the value</td></tr></tbody></table><h2 id="输出门"><a class="anchor" href="#输出门">#</a> 输出门</h2><p><img data-src="assets/image-20230801004103-q1wuomh.png" alt="image"></p><h2 id="总体结构图"><a class="anchor" href="#总体结构图">#</a> 总体结构图</h2><p><img data-src="assets/image-20230801004233-varpage.png" alt="image"></p><h2 id="梯度信息"><a class="anchor" href="#梯度信息">#</a> 梯度信息</h2><p>RNN 网络中会有梯度弥散的情况发生，根本原因就是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">W_R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.00773em">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 的连乘，而 LSTM 就很好的解决了这个问题</p><p><img data-src="assets/image-20230801004435-ay2ogin.png" alt="image"></p><h2 id="相关-api"><a class="anchor" href="#相关-api">#</a> 相关 API</h2><blockquote><p>参考：<a href="assets/52-20230801004608-a4q3xgt.pdf">LSTM 使用.pdf</a></p></blockquote><p><img data-src="assets/image-20230801004548-4qc23n6.png" alt="image"></p><h2 id="情感分类实战"><a class="anchor" href="#情感分类实战">#</a> 情感分类实战</h2><blockquote><p>参考：<a href="assets/53-20230801004921-1as17pj.pdf">情感分类实战.pdf</a></p></blockquote><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># -*- coding: utf-8 -*-</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token triple-quoted-string string">"""lstm</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>Automatically generated by Colaboratory.</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>Original file is located at</pre></td></tr><tr><td data-num="7"></td><td><pre>    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu</pre></td></tr><tr><td data-num="8"></td><td><pre>"""</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>!pip install torch</pre></td></tr><tr><td data-num="11"></td><td><pre>!pip install torchtext</pre></td></tr><tr><td data-num="12"></td><td><pre>!python <span class="token operator">-</span>m spacy download en</pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment"># K80 gpu for 12 hours</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token keyword">import</span> torch</pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token punctuation">,</span> optim</pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data<span class="token punctuation">,</span> datasets</pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'GPU:'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize<span class="token operator">=</span><span class="token string">'spacy'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'len of train data:'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'len of test data:'</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre></pre></td></tr><tr><td data-num="30"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">.</span>label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="32"></td><td><pre></pre></td></tr><tr><td data-num="33"></td><td><pre><span class="token comment"># word2vec, glove</span></pre></td></tr><tr><td data-num="34"></td><td><pre>TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> vectors<span class="token operator">=</span><span class="token string">'glove.6B.100d'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre></pre></td></tr><tr><td data-num="37"></td><td><pre></pre></td></tr><tr><td data-num="38"></td><td><pre>batchsz <span class="token operator">=</span> <span class="token number">30</span></pre></td></tr><tr><td data-num="39"></td><td><pre>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="40"></td><td><pre>train_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span></pre></td></tr><tr><td data-num="41"></td><td><pre>    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="42"></td><td><pre>    batch_size <span class="token operator">=</span> batchsz<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="43"></td><td><pre>    device<span class="token operator">=</span>device</pre></td></tr><tr><td data-num="44"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="45"></td><td><pre></pre></td></tr><tr><td data-num="46"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="47"></td><td><pre>  </pre></td></tr><tr><td data-num="48"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="49"></td><td><pre>        <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="50"></td><td><pre>        """</pre></td></tr><tr><td data-num="51"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span>RNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="52"></td><td><pre>    </pre></td></tr><tr><td data-num="53"></td><td><pre>        <span class="token comment"># [0-10001] => [100]</span></pre></td></tr><tr><td data-num="54"></td><td><pre>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="55"></td><td><pre>        <span class="token comment"># [100] => [256]</span></pre></td></tr><tr><td data-num="56"></td><td><pre>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> </pre></td></tr><tr><td data-num="57"></td><td><pre>                           bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="58"></td><td><pre>        <span class="token comment"># [256*2] => [1]</span></pre></td></tr><tr><td data-num="59"></td><td><pre>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="60"></td><td><pre>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="61"></td><td><pre>    </pre></td></tr><tr><td data-num="62"></td><td><pre>    </pre></td></tr><tr><td data-num="63"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="64"></td><td><pre>        <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="65"></td><td><pre>        x: [seq_len, b] vs [b, 3, 28, 28]</pre></td></tr><tr><td data-num="66"></td><td><pre>        """</pre></td></tr><tr><td data-num="67"></td><td><pre>        <span class="token comment"># [seq, b, 1] => [seq, b, 100]</span></pre></td></tr><tr><td data-num="68"></td><td><pre>        embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="69"></td><td><pre>    </pre></td></tr><tr><td data-num="70"></td><td><pre>        <span class="token comment"># output: [seq, b, hid_dim*2]</span></pre></td></tr><tr><td data-num="71"></td><td><pre>        <span class="token comment"># hidden/h: [num_layers*2, b, hid_dim]</span></pre></td></tr><tr><td data-num="72"></td><td><pre>        <span class="token comment"># cell/c: [num_layers*2, b, hid_di]</span></pre></td></tr><tr><td data-num="73"></td><td><pre>        output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedding<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="74"></td><td><pre>    </pre></td></tr><tr><td data-num="75"></td><td><pre>        <span class="token comment"># [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]</span></pre></td></tr><tr><td data-num="76"></td><td><pre>        hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="77"></td><td><pre>    </pre></td></tr><tr><td data-num="78"></td><td><pre>        <span class="token comment"># [b, hid_dim*2] => [b, 1]</span></pre></td></tr><tr><td data-num="79"></td><td><pre>        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="80"></td><td><pre>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="81"></td><td><pre>    </pre></td></tr><tr><td data-num="82"></td><td><pre>        <span class="token keyword">return</span> out</pre></td></tr><tr><td data-num="83"></td><td><pre></pre></td></tr><tr><td data-num="84"></td><td><pre>rnn <span class="token operator">=</span> RNN<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="85"></td><td><pre></pre></td></tr><tr><td data-num="86"></td><td><pre>pretrained_embedding <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors</pre></td></tr><tr><td data-num="87"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'pretrained_embedding:'</span><span class="token punctuation">,</span> pretrained_embedding<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="88"></td><td><pre>rnn<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>pretrained_embedding<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="89"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'embedding layer inited.'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="90"></td><td><pre></pre></td></tr><tr><td data-num="91"></td><td><pre>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>rnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="92"></td><td><pre>criteon <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="93"></td><td><pre>rnn<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="94"></td><td><pre></pre></td></tr><tr><td data-num="95"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="96"></td><td><pre></pre></td></tr><tr><td data-num="97"></td><td><pre><span class="token keyword">def</span> <span class="token function">binary_acc</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="98"></td><td><pre>    <span class="token triple-quoted-string string">"""</span></pre></td></tr><tr><td data-num="99"></td><td><pre>    get accuracy</pre></td></tr><tr><td data-num="100"></td><td><pre>    """</pre></td></tr><tr><td data-num="101"></td><td><pre>    preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="102"></td><td><pre>    correct <span class="token operator">=</span> torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="103"></td><td><pre>    acc <span class="token operator">=</span> correct<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>correct<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="104"></td><td><pre>    <span class="token keyword">return</span> acc</pre></td></tr><tr><td data-num="105"></td><td><pre></pre></td></tr><tr><td data-num="106"></td><td><pre><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>rnn<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criteon<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="107"></td><td><pre>  </pre></td></tr><tr><td data-num="108"></td><td><pre>    avg_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="109"></td><td><pre>    rnn<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="110"></td><td><pre>  </pre></td></tr><tr><td data-num="111"></td><td><pre>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="112"></td><td><pre>    </pre></td></tr><tr><td data-num="113"></td><td><pre>        <span class="token comment"># [seq, b] => [b, 1] => [b]</span></pre></td></tr><tr><td data-num="114"></td><td><pre>        pred <span class="token operator">=</span> rnn<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="115"></td><td><pre>        <span class="token comment"># </span></pre></td></tr><tr><td data-num="116"></td><td><pre>        loss <span class="token operator">=</span> criteon<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="117"></td><td><pre>        acc <span class="token operator">=</span> binary_acc<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="118"></td><td><pre>        avg_acc<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="119"></td><td><pre>    </pre></td></tr><tr><td data-num="120"></td><td><pre>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="121"></td><td><pre>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="122"></td><td><pre>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="123"></td><td><pre>    </pre></td></tr><tr><td data-num="124"></td><td><pre>        <span class="token keyword">if</span> i<span class="token operator">%</span><span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="125"></td><td><pre>            <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> acc<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="126"></td><td><pre>    </pre></td></tr><tr><td data-num="127"></td><td><pre>    avg_acc <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>avg_acc<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="128"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'avg acc:'</span><span class="token punctuation">,</span> avg_acc<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="129"></td><td><pre>  </pre></td></tr><tr><td data-num="130"></td><td><pre>  </pre></td></tr><tr><td data-num="131"></td><td><pre><span class="token keyword">def</span> <span class="token function">eval</span><span class="token punctuation">(</span>rnn<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criteon<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="132"></td><td><pre>  </pre></td></tr><tr><td data-num="133"></td><td><pre>    avg_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="134"></td><td><pre>  </pre></td></tr><tr><td data-num="135"></td><td><pre>    rnn<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="136"></td><td><pre>  </pre></td></tr><tr><td data-num="137"></td><td><pre>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="138"></td><td><pre>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="139"></td><td><pre></pre></td></tr><tr><td data-num="140"></td><td><pre>            <span class="token comment"># [b, 1] => [b]</span></pre></td></tr><tr><td data-num="141"></td><td><pre>            pred <span class="token operator">=</span> rnn<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="142"></td><td><pre></pre></td></tr><tr><td data-num="143"></td><td><pre>            <span class="token comment">#</span></pre></td></tr><tr><td data-num="144"></td><td><pre>            loss <span class="token operator">=</span> criteon<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="145"></td><td><pre></pre></td></tr><tr><td data-num="146"></td><td><pre>            acc <span class="token operator">=</span> binary_acc<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="147"></td><td><pre>            avg_acc<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="148"></td><td><pre>    </pre></td></tr><tr><td data-num="149"></td><td><pre>    avg_acc <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>avg_acc<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="150"></td><td><pre>  </pre></td></tr><tr><td data-num="151"></td><td><pre>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'>>test:'</span><span class="token punctuation">,</span> avg_acc<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="152"></td><td><pre></pre></td></tr><tr><td data-num="153"></td><td><pre><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="154"></td><td><pre>  </pre></td></tr><tr><td data-num="155"></td><td><pre>    <span class="token builtin">eval</span><span class="token punctuation">(</span>rnn<span class="token punctuation">,</span> test_iterator<span class="token punctuation">,</span> criteon<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="156"></td><td><pre>    train<span class="token punctuation">(</span>rnn<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criteon<span class="token punctuation">)</span></pre></td></tr></table></figure><div class="tags"><a href="/tags/PyTorch/" rel="tag"><i class="ic i-tag"></i> PyTorch</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-06-21 16:43:36" itemprop="dateModified" datetime="2024-06-21T16:43:36+08:00">2024-06-21</time> </span><span id="artificial-intelligence/pytorch/20230731-2/" class="item leancloud_visitors" data-flag-title="PyTorch学习笔记（10）长短期神经网络LSTM" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="宇凌喵 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="宇凌喵 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>宇凌喵 <i class="ic i-at"><em>@</em></i>学无止境</li><li class="link"><strong>本文链接：</strong> <a href="https://blog.aayu.today/artificial-intelligence/pytorch/20230731-2/" title="PyTorch学习笔记（10）长短期神经网络LSTM">https://blog.aayu.today/artificial-intelligence/pytorch/20230731-2/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/artificial-intelligence/pytorch/20230731/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;image.aayu.today&#x2F;uploads&#x2F;2022&#x2F;08&#x2F;22&#x2F;c6b8e6ffdbc55.jpg" title="PyTorch学习笔记（9）循环神经网络RNN"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 机器学习修炼之PyTorch</span><h3>PyTorch学习笔记（9）循环神经网络RNN</h3></a></div><div class="item right"><a href="/artificial-intelligence/pytorch/20230801/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;image.aayu.today&#x2F;uploads&#x2F;2023&#x2F;08&#x2F;06&#x2F;202308062335066.png" title="PyTorch学习笔记（11）自编码器Auto-Encoders"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 机器学习修炼之PyTorch</span><h3>PyTorch学习笔记（11）自编码器Auto-Encoders</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%97%E5%BF%98%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">遗忘门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">输入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E9%97%A8"><span class="toc-number">3.</span> <span class="toc-text">输出门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="toc-number">4.</span> <span class="toc-text">总体结构图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%BF%A1%E6%81%AF"><span class="toc-number">5.</span> <span class="toc-text">梯度信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3-api"><span class="toc-number">6.</span> <span class="toc-text">相关 API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98"><span class="toc-number">7.</span> <span class="toc-text">情感分类实战</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/artificial-intelligence/pytorch/20200602/" rel="bookmark" title="Pytorch-Pytorch安装">Pytorch-Pytorch安装</a></li><li><a href="/artificial-intelligence/pytorch/20200606/" rel="bookmark" title="Pytorch-制作自己的多标签数据集">Pytorch-制作自己的多标签数据集</a></li><li><a href="/artificial-intelligence/pytorch/20200607-1/" rel="bookmark" title="Pytorch-Dropout用法">Pytorch-Dropout用法</a></li><li><a href="/artificial-intelligence/pytorch/20200607-2/" rel="bookmark" title="Pytorch-多输出回归任务实战">Pytorch-多输出回归任务实战</a></li><li><a href="/artificial-intelligence/pytorch/20200610/" rel="bookmark" title="Pytorch-多输出回归任务实战（二）">Pytorch-多输出回归任务实战（二）</a></li><li><a href="/artificial-intelligence/pytorch/20210311/" rel="bookmark" title="Pytorch-常用的交叉熵损失函数CrossEntropyLoss详解">Pytorch-常用的交叉熵损失函数CrossEntropyLoss详解</a></li><li><a href="/artificial-intelligence/pytorch/20210312/" rel="bookmark" title="Pytorch-nn.Softmax函数详解">Pytorch-nn.Softmax函数详解</a></li><li><a href="/artificial-intelligence/pytorch/20210412-1/" rel="bookmark" title="20210412-原理-梯度弥散和梯度爆炸">20210412-原理-梯度弥散和梯度爆炸</a></li><li><a href="/artificial-intelligence/pytorch/20210412-2/" rel="bookmark" title="原理-详解sigmoid与softmax，多分类与多标签分类">原理-详解sigmoid与softmax，多分类与多标签分类</a></li><li><a href="/artificial-intelligence/pytorch/20230716/" rel="bookmark" title="PyTorch学习笔记（1）">PyTorch学习笔记（1）</a></li><li><a href="/artificial-intelligence/pytorch/20230717/" rel="bookmark" title="PyTorch学习笔记（2）">PyTorch学习笔记（2）</a></li><li><a href="/artificial-intelligence/pytorch/20230717-2/" rel="bookmark" title="PyTorch学习笔记（3）随机梯度下降">PyTorch学习笔记（3）随机梯度下降</a></li><li><a href="/artificial-intelligence/pytorch/20230718/" rel="bookmark" title="PyTorch学习笔记（4）神经网络与全连接层">PyTorch学习笔记（4）神经网络与全连接层</a></li><li><a href="/artificial-intelligence/pytorch/20230718-2/" rel="bookmark" title="PyTorch学习笔记（5）过拟合问题">PyTorch学习笔记（5）过拟合问题</a></li><li><a href="/artificial-intelligence/pytorch/20230719/" rel="bookmark" title="PyTorch学习笔记（6）卷积神经网络">PyTorch学习笔记（6）卷积神经网络</a></li><li><a href="/artificial-intelligence/pytorch/20230719-2/" rel="bookmark" title="PyTorch学习笔记（7）ResNet实战">PyTorch学习笔记（7）ResNet实战</a></li><li><a href="/artificial-intelligence/pytorch/20230719-3/" rel="bookmark" title="PyTorch学习笔记（8）迁移学习实战">PyTorch学习笔记（8）迁移学习实战</a></li><li><a href="/artificial-intelligence/pytorch/20230731/" rel="bookmark" title="PyTorch学习笔记（9）循环神经网络RNN">PyTorch学习笔记（9）循环神经网络RNN</a></li><li class="active"><a href="/artificial-intelligence/pytorch/20230731-2/" rel="bookmark" title="PyTorch学习笔记（10）长短期神经网络LSTM">PyTorch学习笔记（10）长短期神经网络LSTM</a></li><li><a href="/artificial-intelligence/pytorch/20230801/" rel="bookmark" title="PyTorch学习笔记（11）自编码器Auto-Encoders">PyTorch学习笔记（11）自编码器Auto-Encoders</a></li><li><a href="/artificial-intelligence/pytorch/20230802/" rel="bookmark" title="PyTorch学习笔记（12）对抗生成网络GAN">PyTorch学习笔记（12）对抗生成网络GAN</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="宇凌喵" data-src="/images/avatar.jpg"><p class="name" itemprop="name">宇凌喵</p><div class="description" itemprop="description">真理和热爱是吾永生的追求</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">387</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">101</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">135</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lsc2lzbG92ZQ==" title="https:&#x2F;&#x2F;github.com&#x2F;ylsislove"><i class="ic i-github"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20veW91cm5hbWU=" title="https:&#x2F;&#x2F;weibo.com&#x2F;yourname"><i class="ic i-weibo"></i></span> <span class="exturl item email" data-url="bWFpbHRvOjkxNjQ5MTAxM0BxcS5jb20=" title="mailto:916491013@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-magic"></i>链环</a><ul class="submenu"><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/webstack/" rel="section"><i class="ic i-star"></i>网址</a></li></ul></li><li class="item"><span class="exturl" data-url="aHR0cHM6Ly9mb3JldmVyYmxvZy5jbi9nby5odG1s"><i class="ic i-paper-plane"></i>虫洞</span></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/artificial-intelligence/pytorch/20230731/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/artificial-intelligence/pytorch/20230801/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/artificial-intelligence/" title="分类于 人工智能">人工智能</a> <i class="ic i-angle-right"></i> <a href="/categories/artificial-intelligence/opencv/" title="分类于 OpenCV">OpenCV</a> <i class="ic i-angle-right"></i> <a href="/categories/artificial-intelligence/opencv/getting-started/" title="分类于 入门">入门</a></div><span><a href="/artificial-intelligence/opencv/getting-started/20201119/" title="OpenCV4（13）-图像翻转（C++，Python，JS）">OpenCV4（13）-图像翻转（C++，Python，JS）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/basic-subjects/" title="分类于 基础学科">基础学科</a> <i class="ic i-angle-right"></i> <a href="/categories/basic-subjects/mathematics/" title="分类于 数学">数学</a> <i class="ic i-angle-right"></i> <a href="/categories/basic-subjects/mathematics/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/" title="分类于 高等数学">高等数学</a></div><span><a href="/basic-subjects/mathematics/advanced-mathematics/20220312/" title="高等数学-常用公式">高等数学-常用公式</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/front-end/" title="分类于 前端系列">前端系列</a> <i class="ic i-angle-right"></i> <a href="/categories/front-end/es567/" title="分类于 ECMAScript">ECMAScript</a></div><span><a href="/front-end/es567/20200528/" title="前端-ES6知识点总结（三）">前端-ES6知识点总结（三）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/gis/" title="分类于 GIS开发笔记">GIS开发笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/gis/arcgis/" title="分类于 ArcGIS">ArcGIS</a></div><span><a href="/gis/arcgis/20200909/" title="GIS小白教程：如何利用高程DEM数据构建三维地图模型（基于ArcScene）">GIS小白教程：如何利用高程DEM数据构建三维地图模型（基于ArcScene）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/english/" title="分类于 英语学习之路">英语学习之路</a> <i class="ic i-angle-right"></i> <a href="/categories/english/ielts/" title="分类于 雅思备战">雅思备战</a></div><span><a href="/english/ielts/20200717/" title="雅思写作-常用谚语（二）">雅思写作-常用谚语（二）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/english/" title="分类于 英语学习之路">英语学习之路</a> <i class="ic i-angle-right"></i> <a href="/categories/english/daily/" title="分类于 日常小知识点">日常小知识点</a></div><span><a href="/english/daily/20200602/" title="英语-震惊！这些英语其实在骂你？">英语-震惊！这些英语其实在骂你？</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据学习笔记">大数据学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/others/" title="分类于 其他">其他</a></div><span><a href="/big-data/others/20200618-2/" title="GeoMesa-导入GDELT数据到HBase">GeoMesa-导入GDELT数据到HBase</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/english/" title="分类于 英语学习之路">英语学习之路</a> <i class="ic i-angle-right"></i> <a href="/categories/english/ielts/" title="分类于 雅思备战">雅思备战</a></div><span><a href="/english/ielts/20200716/" title="雅思写作-常用谚语（一）">雅思写作-常用谚语（一）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/artificial-intelligence/" title="分类于 人工智能">人工智能</a> <i class="ic i-angle-right"></i> <a href="/categories/artificial-intelligence/opencv/" title="分类于 OpenCV">OpenCV</a> <i class="ic i-angle-right"></i> <a href="/categories/artificial-intelligence/opencv/getting-started/" title="分类于 入门">入门</a></div><span><a href="/artificial-intelligence/opencv/getting-started/20201108/" title="OpenCV4（2）-保存图像与色彩空间的转换（C++，Python，JS）">OpenCV4（2）-保存图像与色彩空间的转换（C++，Python，JS）</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/hardware/" title="分类于 硬件修炼手册">硬件修炼手册</a> <i class="ic i-angle-right"></i> <a href="/categories/hardware/iot/" title="分类于 物联网">物联网</a></div><span><a href="/hardware/iot/20220920/" title="移远BC28模块连接华为云IoT平台">移远BC28模块连接华为云IoT平台</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2020 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">宇凌喵 @ Aayu Yain</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">823k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">12:29</span> <span class="beian"><i class="ic i-beian1"></i> </span><span>晋ICP备19006357号-4</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"artificial-intelligence/pytorch/20230731-2/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="assets/polyfill.js"></script><script src="/assets/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->